{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MCIT AWS Machine Learning Developer Program</center>\n",
    "## <center> Use case 1: Tabular Dataset </center>\n",
    "## <center> Default of Credit Card Clients </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Introduction</a></li>\n",
    "<li><a href=\"#Read\">Read and view the data </a></li>    \n",
    "<li><a href=\"#Prepare\">Preparing the data</a></li>\n",
    "    - <a href=\"#tran\">Data types, missing and duplicate values </a><br>\n",
    "    - <a href=\"#Explore\">Featureset Exploration </a><br>\n",
    "    - <a href=\"#tran\">Transforming Skewed Continuous Features </a><br>\n",
    "    - <a href=\"#norm\">Normalizing Numerical Features </a><br>\n",
    "    - <a href=\"#ohe\">One-hot Encoding </a><br>\n",
    "    - <a href=\"#sh\">Shuffle and Split Data </a><br>\n",
    "    \n",
    "<li><a href=\"#Evaluate\">Evaluating Model Performance</a></li>\n",
    "    - <a href=\"#Metrics\">Metrics and the Naive Predictor </a><br>\n",
    "    - <a href=\"#NaivePer\">Naive Predictor Performace </a><br>\n",
    "    - <a href=\"#Supervised\">Supervised Learning Models </a><br>\n",
    "    - <a href=\"#App\">Model Application </a><br>\n",
    "    - <a href=\"#pipe\">Creating a Training and Predicting Pipeline </a><br>\n",
    "    - <a href=\"#init_eval\">Initial Model Evaluation </a><br>\n",
    "    \n",
    "    \n",
    "<li><a href=\"#Improve\">Improving the results</a></li>\n",
    "    - <a href=\"#Best\"> Choosing the best model </a><br>\n",
    "    - <a href=\"#Describe\"> Describing the Model in Layman's Terms</a><br>\n",
    "    - <a href=\"#Tuning\"> Model Tuning</a><br>\n",
    "    - <a href=\"#Final_Eval\"> Final Model Evaluation</a><br>\n",
    "  \n",
    "<li><a href=\"#Feature\">Feature Selection</a></li>\n",
    "    - <a href=\"#f_obs\"> Feature Relevance Observation </a><br>\n",
    "    - <a href=\"#f_ext\"> Extracting Feature Importance</a><br>\n",
    "    - <a href=\"#f_sel\"> Feature Selection</a><br>\n",
    "    - <a href=\"#f_eff\"> Effects of Feature Selection</a><br>\n",
    "\n",
    "<li><a href=\"#kaggle\">Kaggle Submission</a></li>\n",
    "    - <a href=\"#check\"> Check missing data </a><br>\n",
    "    - <a href=\"#deal\"> Dealing with missing values</a><br>\n",
    "    - <a href=\"#write\"> Write the output file </a><br>\n",
    "<li><a href=\"#Conc\">Conclusion and future work</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"intro\"></a>\n",
    "## Introduction\n",
    "\n",
    "In this project, **_several Machine Learning supervised algorithms is implemented_** to accurately predict customer default for a credit card issuer. Having _compared those elementary approaches to a naive predictor_, the best candidate algorithm is chosen from preliminary results. Finally,**_further optimization to this algorithm is done_**. _**<u>Our goal</u>**_ with this implementation is to **construct a model that accurately predicts whether an individual is going to delay the required payment in the next month or not**. This sort of task can arise in a credit card issuer settings where the accurate predictions of customer habits might be crucial for their financial situation.  \n",
    "\n",
    "The dataset for this project originates from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/default+of+credit+card+clients) which uploaded by I-Cheng Yeh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import seaborn as sns\n",
    "# Import supplementary visualization code visuals.py\n",
    "import visuals as vs\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "submit = False\n",
    "# Import sklearn.preprocessing.StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# Import two metrics from sklearn - fbeta_score and accuracy_score\n",
    "from sklearn.metrics import accuracy_score, fbeta_score\n",
    "# Import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from IPython.display import display, HTML\n",
    "from dython.nominal import associations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Read\"></a>\n",
    "----\n",
    "## Read and view the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X15</th>\n",
       "      <th>X16</th>\n",
       "      <th>X17</th>\n",
       "      <th>X18</th>\n",
       "      <th>X19</th>\n",
       "      <th>X20</th>\n",
       "      <th>X21</th>\n",
       "      <th>X22</th>\n",
       "      <th>X23</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ID</th>\n",
       "      <td>LIMIT_BAL</td>\n",
       "      <td>SEX</td>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>MARRIAGE</td>\n",
       "      <td>AGE</td>\n",
       "      <td>PAY_0</td>\n",
       "      <td>PAY_2</td>\n",
       "      <td>PAY_3</td>\n",
       "      <td>PAY_4</td>\n",
       "      <td>PAY_5</td>\n",
       "      <td>...</td>\n",
       "      <td>BILL_AMT4</td>\n",
       "      <td>BILL_AMT5</td>\n",
       "      <td>BILL_AMT6</td>\n",
       "      <td>PAY_AMT1</td>\n",
       "      <td>PAY_AMT2</td>\n",
       "      <td>PAY_AMT3</td>\n",
       "      <td>PAY_AMT4</td>\n",
       "      <td>PAY_AMT5</td>\n",
       "      <td>PAY_AMT6</td>\n",
       "      <td>default payment next month</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1   X2         X3        X4   X5     X6     X7     X8     X9  \\\n",
       "ID  LIMIT_BAL  SEX  EDUCATION  MARRIAGE  AGE  PAY_0  PAY_2  PAY_3  PAY_4   \n",
       "\n",
       "      X10  ...        X15        X16        X17       X18       X19       X20  \\\n",
       "ID  PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3   \n",
       "\n",
       "         X21       X22       X23                           Y  \n",
       "ID  PAY_AMT4  PAY_AMT5  PAY_AMT6  default payment next month  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Census dataset\n",
    "data = pd.read_excel('default of credit card clients.xls', index_col = 0)\n",
    "# Success - Display the first record\n",
    "display(data.head(n=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Prepare\"></a>\n",
    "----\n",
    "## Preparing the Data\n",
    "Before the data can be used as an input for machine learning algorithms, it often must be cleaned, formatted, and restructured — this is typically known as **preprocessing**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data types, missing and duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30001 entries, ID to 30000\n",
      "Data columns (total 24 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   X1      30001 non-null  object\n",
      " 1   X2      30001 non-null  object\n",
      " 2   X3      29670 non-null  object\n",
      " 3   X4      29678 non-null  object\n",
      " 4   X5      30001 non-null  object\n",
      " 5   X6      30001 non-null  object\n",
      " 6   X7      30001 non-null  object\n",
      " 7   X8      30001 non-null  object\n",
      " 8   X9      30001 non-null  object\n",
      " 9   X10     30001 non-null  object\n",
      " 10  X11     30001 non-null  object\n",
      " 11  X12     30001 non-null  object\n",
      " 12  X13     30001 non-null  object\n",
      " 13  X14     30001 non-null  object\n",
      " 14  X15     30001 non-null  object\n",
      " 15  X16     30001 non-null  object\n",
      " 16  X17     30001 non-null  object\n",
      " 17  X18     30001 non-null  object\n",
      " 18  X19     30001 non-null  object\n",
      " 19  X20     30001 non-null  object\n",
      " 20  X21     30001 non-null  object\n",
      " 21  X22     30001 non-null  object\n",
      " 22  X23     30001 non-null  object\n",
      " 23  Y       30001 non-null  object\n",
      "dtypes: object(24)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1       0\n",
       "X2       0\n",
       "X3     331\n",
       "X4     323\n",
       "X5       0\n",
       "X6       0\n",
       "X7       0\n",
       "X8       0\n",
       "X9       0\n",
       "X10      0\n",
       "X11      0\n",
       "X12      0\n",
       "X13      0\n",
       "X14      0\n",
       "X15      0\n",
       "X16      0\n",
       "X17      0\n",
       "X18      0\n",
       "X19      0\n",
       "X20      0\n",
       "X21      0\n",
       "X22      0\n",
       "X23      0\n",
       "Y        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X1        82\n",
       "X2         3\n",
       "X3         6\n",
       "X4         4\n",
       "X5        64\n",
       "X6        12\n",
       "X7        12\n",
       "X8        12\n",
       "X9        12\n",
       "X10       11\n",
       "X11       11\n",
       "X12    22724\n",
       "X13    22347\n",
       "X14    22027\n",
       "X15    21549\n",
       "X16    21011\n",
       "X17    20605\n",
       "X18     7944\n",
       "X19     7900\n",
       "X20     7519\n",
       "X21     6938\n",
       "X22     6898\n",
       "X23     6940\n",
       "Y          3\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the dataset defines all the features as `object`, it can be noticed ,from the number of unique values as well the provided meta data , that there are some categorical and numerical features. Thus, the data is reformatted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>ID</th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>AGE</th>\n",
       "      <th>BILL_AMT1</th>\n",
       "      <th>BILL_AMT2</th>\n",
       "      <th>BILL_AMT3</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>...</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>PAY_6</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20000.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>3913.0</td>\n",
       "      <td>3102.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>689.0</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>married</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-2</td>\n",
       "      <td>-2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>120000.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>1725.0</td>\n",
       "      <td>2682.0</td>\n",
       "      <td>3272.0</td>\n",
       "      <td>3455.0</td>\n",
       "      <td>3261.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>female</td>\n",
       "      <td>university</td>\n",
       "      <td>single</td>\n",
       "      <td>-1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "ID  LIMIT_BAL   AGE  BILL_AMT1  BILL_AMT2  BILL_AMT3  BILL_AMT4  BILL_AMT5  \\\n",
       "1     20000.0  24.0     3913.0     3102.0      689.0        0.0        0.0   \n",
       "2    120000.0  26.0     2682.0     1725.0     2682.0     3272.0     3455.0   \n",
       "\n",
       "ID  BILL_AMT6  PAY_AMT1  PAY_AMT2  ...     SEX   EDUCATION  MARRIAGE  PAY_0  \\\n",
       "1         0.0       0.0     689.0  ...  female  university   married      2   \n",
       "2      3261.0       0.0    1000.0  ...  female  university    single     -1   \n",
       "\n",
       "ID PAY_2 PAY_3 PAY_4 PAY_5 PAY_6  Y  \n",
       "1      2    -1    -1    -2    -2  1  \n",
       "2      2     0     0     0     2  1  \n",
       "\n",
       "[2 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract Categorical data\n",
    "\n",
    "Categorical_data = data[['X' + str(i) for i in range(2,5)] + ['X' + str(i) for i in range(6,12)] + ['Y']]\n",
    "Categorical_data.columns = Categorical_data.loc['ID']\n",
    "Categorical_data.drop(index = 'ID', inplace = True)\n",
    "\n",
    "# Extract Numerical data \n",
    "\n",
    "numerical_data = data[['X1','X5']+ ['X' + str(i) for i in range(12,24)]]\n",
    "numerical_data.columns = numerical_data.loc['ID']\n",
    "numerical_data.drop(index = 'ID', inplace = True)\n",
    "numerical_data = numerical_data.astype(float)\n",
    "\n",
    "# Recombine the data \n",
    "data = pd.concat([numerical_data,Categorical_data],axis = 1)\n",
    "data.rename(columns={\"default payment next month\": \"Y\"}, inplace = True)\n",
    "\n",
    "# View the data \n",
    "display(data.head(n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30000 entries, 1 to 30000\n",
      "Data columns (total 24 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   LIMIT_BAL  30000 non-null  float64\n",
      " 1   AGE        30000 non-null  float64\n",
      " 2   BILL_AMT1  30000 non-null  float64\n",
      " 3   BILL_AMT2  30000 non-null  float64\n",
      " 4   BILL_AMT3  30000 non-null  float64\n",
      " 5   BILL_AMT4  30000 non-null  float64\n",
      " 6   BILL_AMT5  30000 non-null  float64\n",
      " 7   BILL_AMT6  30000 non-null  float64\n",
      " 8   PAY_AMT1   30000 non-null  float64\n",
      " 9   PAY_AMT2   30000 non-null  float64\n",
      " 10  PAY_AMT3   30000 non-null  float64\n",
      " 11  PAY_AMT4   30000 non-null  float64\n",
      " 12  PAY_AMT5   30000 non-null  float64\n",
      " 13  PAY_AMT6   30000 non-null  float64\n",
      " 14  SEX        30000 non-null  object \n",
      " 15  EDUCATION  29669 non-null  object \n",
      " 16  MARRIAGE   29677 non-null  object \n",
      " 17  PAY_0      30000 non-null  object \n",
      " 18  PAY_2      30000 non-null  object \n",
      " 19  PAY_3      30000 non-null  object \n",
      " 20  PAY_4      30000 non-null  object \n",
      " 21  PAY_5      30000 non-null  object \n",
      " 22  PAY_6      30000 non-null  object \n",
      " 23  Y          30000 non-null  object \n",
      "dtypes: float64(14), object(10)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2212"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Y.sum() / len(data.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace = True, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 29351 entries, 1 to 30000\n",
      "Data columns (total 24 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   LIMIT_BAL  29351 non-null  float64\n",
      " 1   AGE        29351 non-null  float64\n",
      " 2   BILL_AMT1  29351 non-null  float64\n",
      " 3   BILL_AMT2  29351 non-null  float64\n",
      " 4   BILL_AMT3  29351 non-null  float64\n",
      " 5   BILL_AMT4  29351 non-null  float64\n",
      " 6   BILL_AMT5  29351 non-null  float64\n",
      " 7   BILL_AMT6  29351 non-null  float64\n",
      " 8   PAY_AMT1   29351 non-null  float64\n",
      " 9   PAY_AMT2   29351 non-null  float64\n",
      " 10  PAY_AMT3   29351 non-null  float64\n",
      " 11  PAY_AMT4   29351 non-null  float64\n",
      " 12  PAY_AMT5   29351 non-null  float64\n",
      " 13  PAY_AMT6   29351 non-null  float64\n",
      " 14  SEX        29351 non-null  object \n",
      " 15  EDUCATION  29351 non-null  object \n",
      " 16  MARRIAGE   29351 non-null  object \n",
      " 17  PAY_0      29351 non-null  object \n",
      " 18  PAY_2      29351 non-null  object \n",
      " 19  PAY_3      29351 non-null  object \n",
      " 20  PAY_4      29351 non-null  object \n",
      " 21  PAY_5      29351 non-null  object \n",
      " 22  PAY_6      29351 non-null  object \n",
      " 23  Y          29351 non-null  object \n",
      "dtypes: float64(14), object(10)\n",
      "memory usage: 5.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22234336138462063"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Y.sum() / len(data.Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.index.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small fraction of the rows are missing their values for `Education` and `marriage` columns ( 331 out of 30,000 or 1.10%). Thus, there is no need for `imputation` since dropping the missing rows will not harm the data fed to the model too much or even affect the target column balance. Furthermore, the data represents unique set of customers as indicated by the index column `ID`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "female    17713\n",
       "male      11638\n",
       "Name: SEX, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['SEX'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "university         13868\n",
       "graduate school    10535\n",
       "high school         4813\n",
       "others               121\n",
       "0                     14\n",
       "Name: EDUCATION, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['EDUCATION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Explore'></a>\n",
    "### Featureset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SEX</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>female</th>\n",
       "      <td>17713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>male</th>\n",
       "      <td>17713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   % observations\n",
       "SEX                   \n",
       "female           17713\n",
       "male             17713"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graduate school</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>high school</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>others</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>university</th>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0            % observations\n",
       "EDUCATION                      \n",
       "0                            14\n",
       "graduate school              14\n",
       "high school                  14\n",
       "others                       14\n",
       "university                   14"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>married</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     % observations\n",
       "MARRIAGE                \n",
       "0                     54\n",
       "married               54\n",
       "single                54"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "PAY_0                \n",
       "-2               2704\n",
       "-1               2704\n",
       " 0               2704\n",
       " 1               2704\n",
       " 2               2704\n",
       " 3               2704\n",
       " 4               2704\n",
       " 5               2704\n",
       " 6               2704\n",
       " 7               2704\n",
       " 8               2704"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_2</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "PAY_2                \n",
       "-2               3706\n",
       "-1               3706\n",
       " 0               3706\n",
       " 1               3706\n",
       " 2               3706\n",
       " 3               3706\n",
       " 4               3706\n",
       " 5               3706\n",
       " 6               3706\n",
       " 7               3706\n",
       " 8               3706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_3</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "PAY_3                \n",
       "-2               4017\n",
       "-1               4017\n",
       " 0               4017\n",
       " 1               4017\n",
       " 2               4017\n",
       " 3               4017\n",
       " 4               4017\n",
       " 5               4017\n",
       " 6               4017\n",
       " 7               4017\n",
       " 8               4017"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_4</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "PAY_4                \n",
       "-2               4275\n",
       "-1               4275\n",
       " 0               4275\n",
       " 1               4275\n",
       " 2               4275\n",
       " 3               4275\n",
       " 4               4275\n",
       " 5               4275\n",
       " 6               4275\n",
       " 7               4275\n",
       " 8               4275"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_5</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "PAY_5                \n",
       "-2               4467\n",
       "-1               4467\n",
       " 0               4467\n",
       " 2               4467\n",
       " 3               4467\n",
       " 4               4467\n",
       " 5               4467\n",
       " 6               4467\n",
       " 7               4467\n",
       " 8               4467"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAY_6</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-2</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "PAY_6                \n",
       "-2               4796\n",
       "-1               4796\n",
       " 0               4796\n",
       " 2               4796\n",
       " 3               4796\n",
       " 4               4796\n",
       " 5               4796\n",
       " 6               4796\n",
       " 7               4796\n",
       " 8               4796"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>% observations</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Y</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  % observations\n",
       "Y                    \n",
       "0               22825\n",
       "1               22825"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Frequency tables for each categorical feature\n",
    "for column in data.select_dtypes(include=['object']).columns:\n",
    "    display(pd.crosstab(index=data[column], columns='% observations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.3'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Explore numerical features\n",
    "display(data.describe())\n",
    "# Explore categorical features\n",
    "display(data.describe(include=np.object))\n",
    "#%matplotlib inline\n",
    "hist = data.hist(bins=30, sharey=True, figsize=(20, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"tran\"></a>\n",
    "### Transforming Skewed Continuous Features\n",
    "A dataset may sometimes contain at least one feature whose values tend to lie near a single number, but will also have a non-trivial number of vastly larger or smaller values than that single number.  Algorithms can be sensitive to such distributions of values and can underperform if the range is not properly normalized. With the census dataset two features fit this description: '`capital-gain'` and `'capital-loss'`. \n",
    "\n",
    "Run the code cell below to plot a histogram of these two features. Note the range of the values present and how they are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target label\n",
    "default = data['Y']\n",
    "features_raw = data.drop('Y', axis = 1)\n",
    "\n",
    "# Visualize skewed continuous features of original data\n",
    "vs.distribution(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For highly-skewed feature distributions such as `'BILL_AMT'` and `'PAY'`, it is common practice to apply a <a href=\"https://en.wikipedia.org/wiki/Data_transformation_(statistics)\">logarithmic transformation</a> on the data so that the very large and very small values do not negatively affect the performance of a learning algorithm. Using a logarithmic transformation significantly reduces the range of values caused by outliers. Care must be taken when applying this transformation however: The logarithm of `0` is undefined, so we must translate the values by a small amount above `0` to apply the the logarithm successfully.\n",
    "\n",
    "Run the code cell below to perform a transformation on the data and visualize the results. Again, note the range of values and how they are distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = ['BILL_AMT'+ str(i) for i in range(1,7)] + ['PAY_AMT'+ str(i) for i in range(1,7)]\n",
    "features_raw['BILL_AMT1']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Bill Amount has negatives so log is not suitable.\n",
    "# Log-transform the skewed features\n",
    "skewed = ['BILL_AMT'+ str(i) for i in range(1,7)] + ['PAY_AMT'+ str(i) for i in range(1,7)]\n",
    "features_log_transformed = pd.DataFrame(data = features_raw)\n",
    "\n",
    "features_log_transformed[skewed] = features_raw[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# Visualize the new log distributions\n",
    "vs.distribution(features_log_transformed, transformed = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"norm\"></a>\n",
    "### Normalizing Numerical Features\n",
    "In addition to performing transformations on features that are highly skewed, it is often good practice to perform some type of scaling on numerical features. Applying a scaling to the data does not change the shape of each feature's distribution (such as `'capital-gain'` or `'capital-loss'` above); however, normalization ensures that each feature is treated equally when applying supervised learners. Note that once scaling is applied, observing the data in its raw form will no longer have the same original meaning, as exampled below.\n",
    "\n",
    "We will use [`sklearn.preprocessing.MinMaxScaler`](http://scikitlearn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html) to normalize each numerical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize a scaler, then apply it to the features\n",
    "scaler = MinMaxScaler() # default=(0, 1)\n",
    "numerical = [\"LIMIT_BAL\", 'AGE'] + ['BILL_AMT'+str(i) for i in  range(1,7)] + ['PAY_AMT'+str(i) for i in  range(1,7)]\n",
    "\n",
    "features_log_minmax_transform = pd.DataFrame(data = features_raw)\n",
    "features_log_minmax_transform[numerical] = scaler.fit_transform(features_raw[numerical])\n",
    "\n",
    "# Show an example of a record with scaling applied\n",
    "display(features_log_minmax_transform.head(n = 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ohe\"></a>\n",
    "### One-hot Encoding\n",
    "\n",
    "From the table in **Exploring the Data** above, we can see there are several features for each record that are non-numeric. Typically, learning algorithms expect input to be numeric, which requires that non-numeric features (called *categorical variables*) be converted. One popular way to convert categorical variables is by using the **one-hot encoding** scheme. One-hot encoding creates a _\"dummy\"_ variable for each possible category of each non-numeric feature. For example, assume `someFeature` has three possible entries: `A`, `B`, or `C`. We then encode this feature into `someFeature_A`, `someFeature_B` and `someFeature_C`.\n",
    "\n",
    "|   | someFeature |                    | someFeature_A | someFeature_B | someFeature_C |\n",
    "| :-: | :-: |                            | :-: | :-: | :-: |\n",
    "| 0 |  B  |  | 0 | 1 | 0 |\n",
    "| 1 |  C  | ----> one-hot encode ----> | 0 | 0 | 1 |\n",
    "| 2 |  A  |  | 1 | 0 | 0 |\n",
    "\n",
    "Additionally, as with the non-numeric features, we need to convert the non-numeric target label, `'income'` to numerical values for the learning algorithm to work. Since there are only two possible categories for this label (\"<=50K\" and \">50K\"), we can avoid using one-hot encoding and simply encode these two categories as `0` and `1`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-hot encode the 'features_log_minmax_transform' data using pandas.get_dummies()\n",
    "features_final = pd.get_dummies(features_log_minmax_transform)\n",
    "\n",
    "# Print the number of features after one-hot encoding\n",
    "encoded = list(features_final.columns)\n",
    "print(\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "\n",
    "# Uncomment the following line to see the encoded feature names\n",
    "print (encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"sh\"></a>\n",
    "### Shuffle and Split Data\n",
    "Now all _categorical variables_ have been converted into numerical features, and all numerical features have been normalized. As always, we will now split the data (both features and their labels) into training and test sets. 80% of the data will be used for training and 20% for testing.\n",
    "\n",
    "Run the code cell below to perform this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_final.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the 'features' and 'income' data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_final, \n",
    "                                                    default, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)\n",
    "\n",
    "# Show the results of the split\n",
    "print(\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print(\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"Evaluate\"></a>\n",
    "----\n",
    "## Evaluating Model Performance\n",
    "In this section, we will investigate four different algorithms, and determine which is best at modeling the data. Three of these algorithms will be supervised learners, and the fourth algorithm is known as a *naive predictor*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Metrics\"></a>\n",
    "### Metrics and the Naive Predictor\n",
    "`CharityML`, equipped with their research, knows individuals that make more than `$50,000` are most likely to donate to their charity. Because of this, `CharityML` is particularly interested in predicting who makes more than `$50,000` accurately. It would seem that using **accuracy** as a metric for evaluating a particular model's performace would be appropriate. Additionally, identifying someone that *does not* make more than `$50,000` as someone who does would be detrimental to `CharityML`, since they are looking to find individuals willing to donate. Therefore, a model's ability to precisely predict those that make more than \\$50,000 is *more important* than the model's ability to **recall** those individuals. We can use **F-beta score** as a metric that considers both precision and recall:\n",
    "\n",
    "$$ F_{\\beta} = (1 + \\beta^2) \\cdot \\frac{precision \\cdot recall}{\\left( \\beta^2 \\cdot precision \\right) + recall} $$\n",
    "\n",
    "In particular, when $\\beta = 0.5$, more emphasis is placed on precision. This is called the **F$_{0.5}$ score** (or F-score for simplicity).\n",
    "\n",
    "Looking at the distribution of classes (those who make at most `$50,000`, and those who make more), it's clear that most individuals do not make more than `$50,000`. This can greatly affect **accuracy**, since we could simply say \"this person does not make more than `$50,000`\" and generally will be right, without ever looking at the data! Making such a statement would be called **naive**, since we have not considered any information to substantiate the claim. It is always important to consider the *naive prediction* for your data, to help establish a benchmark for whether a model is performing well. That been said, using that prediction would be pointless: If we predicted all people made less than `$50,000`, `CharityML` would identify no one as donors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Recap of accuracy, precision, recall\n",
    "\n",
    "**Accuracy** measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "**Precision** tells us what proportion of true positives, actually were true positives.\n",
    "It is a ratio of true positives(indviduals classified as making more than `$50,000` , and which are actually make) to all positives(all wealthy people, irrespective of whether that was the correct classification), it can be expressed as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $Precision=\\ \\frac{True\\ positives}{True\\ positives+False\\ positives} $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall(sensitivity)** tells us what proportion of people that actually were wealthy were classified by us as wealthy.\n",
    "It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $Recall=\\ \\frac{True\\ positives}{True\\ positives + False\\ negatives} $ </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren't, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is the weighted average(harmonic mean) of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score(we take the harmonic mean as we are dealing with ratios)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"NaivePer\"></a>\n",
    "### Naive Predictor Performace\n",
    "If we choose a model that always predict an individual make more than $50,000, we are going to explore what would  that model's accuracy and F-score be on this dataset.\n",
    "\n",
    "The purpose of generating a naive predictor is simply to show what a base model without any intelligence would look like. In the real world, ideally the base model would be either the results of a previous model or could be based on a research paper upon which you are looking to improve. When there is no benchmark model set, getting a result better than random choice is a place we could start from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TP = np.sum(income) # Counting the ones as this is the naive case. Note that 'income' is the 'income_raw' data \n",
    "encoded to numerical values done in the data preprocessing step.\n",
    "FP = income.count() - TP # Specific to the naive case\n",
    "\n",
    "TN = 0 # No predicted negatives in the naive case\n",
    "FN = 0 # No predicted negatives in the naive case\n",
    "'''\n",
    "# Calculate accuracy, precision and recall\n",
    "TP = np.sum(default)\n",
    "FP = len(default) - TP \n",
    "TN = 0\n",
    "FN = 0 \n",
    "\n",
    "accuracy = (TP + TN)/(TP + FP + TN + FN)\n",
    "recall = (TP) / (TP + FN)\n",
    "precision = TP / (TP + FP)\n",
    "\n",
    "# Calculate F-score using the formula above for beta = 0.5 and correct values for precision and recall.\n",
    "beta = .5\n",
    "fscore = (1+ beta**2) * (precision * recall) / (beta **2 * precision + recall)\n",
    "\n",
    "# Print the results \n",
    "print(\"Naive Predictor: [Accuracy score: {:.4f}, F-score: {:.4f}]\".format(accuracy, fscore))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Supervised\"></a>\n",
    "###  Supervised Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"App\"></a>\n",
    "### Model Application\n",
    "List three of the supervised learning models above that are appropriate for this problem that you will test on the census data. For each model chosen\n",
    "\n",
    "- Describe one real-world application in industry where the model can be applied. \n",
    "- What are the strengths of the model; when does it perform well?\n",
    "- What are the weaknesses of the model; when does it perform poorly?\n",
    "- What makes this model a good candidate for the problem, given what you know about the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print(df):\n",
    "    return display(HTML(df.to_html().replace(\"\\\\n\",\"<br>\")))\n",
    "\n",
    "Models_Comparison = pd.DataFrame(columns = ['SVM', 'Random Forest','Naive Bayes'], \n",
    "                                 index = ['Real World Application', 'Pros', 'Cons', 'Candidacy for this data'])\n",
    "\n",
    "Models_Comparison.at['Real World Application', 'SVM'] = 'Image Classifiers'\n",
    "Models_Comparison.at['Real World Application', 'Random Forest'] = 'Recommendation Systems'\n",
    "Models_Comparison.at['Real World Application','Naive Bayes'] = 'Spam Classifiers'\n",
    "\n",
    "Models_Comparison.at['Pros','SVM'] = u\"\\u2022 Kernels\\n\\u2022 Maximize Boundaries\"\n",
    "Models_Comparison.at['Pros','Naive Bayes'] = u\"\\u2022 Ability to handle an extremely large number of features\\n\\\n",
    "\\u2022 Performs well even with the presence of irrelevant features\\n\\\n",
    "\\u2022 Simple, rarely need to tune its parameters\\n\\\n",
    "\\u2022 It rarely ever overfits the data\\n\\\n",
    "\\u2022 Fast training and prediction times\"\n",
    "Models_Comparison.at['Pros', 'Random Forest'] = u\"\\u2022 Not sensitive to outliers\\n\\\n",
    "\\u2022 Handles both categorical and continuous data\"\n",
    "\n",
    "Models_Comparison.at['Cons','SVM'] = u\"\\u2022 Sensitive to outliers i.e. when target classes are overlapping\"\n",
    "Models_Comparison.at['Cons', 'Random Forest'] = u\"\\u2022 Prone to over fitting\"\n",
    "Models_Comparison.at['Cons', 'Naive Bayes'] = u\"\\u2022 Vanishing values due to multiplication of small probabilities\"\n",
    "\n",
    "Models_Comparison.at['Candidacy for this data', 'SVM']= u\"\\u2022 Ability to handle non-linear relationships.\" \n",
    "Models_Comparison.at['Candidacy for this data', 'Random Forest']= u\"\\u2022 It can handle categorical features effectively.\"\n",
    "Models_Comparison.at['Candidacy for this data', 'Naive Bayes']= u\"\\u2022 Fast but accurate classifier.\"\n",
    "\n",
    "References = ['https://medium.com/@anuuz.soni/pros-and-cons-of-naive-bayes-classifier-40b67249ae8',\n",
    "             'https://data-flair.training/blogs/applications-of-svm/',\n",
    "             'https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/',\n",
    "             'https://towardsai.net/p/machine-learning/why-choose-random-forest-and-not-decision-trees']\n",
    "\n",
    "pretty_print(Models_Comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"pipe\"></a>\n",
    "### Creating a Training and Predicting Pipeline\n",
    "To properly evaluate the performance of chosen models, it's important that we create a training and predicting pipeline that allows us to quickly and effectively train models using various sizes of training data and perform predictions on the testing data. This implementation will be used in the following section.\n",
    "\n",
    "In the code block below, we implement the following:\n",
    " - Import `fbeta_score` and `accuracy_score` from [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#sklearn-metrics-metrics).\n",
    " - Fit the learner to the sampled training data and record the training time.\n",
    " - Perform predictions on the test data `X_test`, and also on the first 300 training points `X_train[:300]`.\n",
    "   - Record the total prediction time.\n",
    " - Calculate the accuracy score for both the training subset and testing set.\n",
    " - Calculate the F-score for both the training subset and testing set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict(learner, sample_size, X_train, y_train, X_test, y_test): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_test: features testing set\n",
    "       - y_test: income testing set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Fit the learner to the training data using slicing with 'sample_size' using .fit(training_features[:], training_labels[:])\n",
    "    start = time() # Get start time\n",
    "    learner = learner.fit(X_train.iloc[0:sample_size+1, :], y_train[0:sample_size+1])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the training time\n",
    "    results['train_time'] = end - start\n",
    "        \n",
    "    # Get the predictions on the test set(X_test),\n",
    "    #       then get predictions on the first 300 training samples(X_train) using .predict()\n",
    "    start = time() # Get start time\n",
    "    predictions_test = learner.predict(X_test)\n",
    "    predictions_train = learner.predict(X_train.iloc[0:300, :])\n",
    "    end = time() # Get end time\n",
    "    \n",
    "    # Calculate the total prediction time\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # Compute accuracy on the first 300 training samples which is y_train[:300]\n",
    "    results['acc_train'] = accuracy_score(predictions_train[0:300], y_train[0:300])\n",
    "        \n",
    "    # Compute accuracy on test set using accuracy_score()\n",
    "    results['acc_test'] = accuracy_score(predictions_test, y_test)\n",
    "    \n",
    "    # Compute F-score on the the first 300 training samples using fbeta_score()\n",
    "    results['f_train'] = fbeta_score(predictions_train[0:300], y_train[0:300], beta = .5)\n",
    "        \n",
    "    # Compute F-score on the test set which is y_test\n",
    "    results['f_test'] = fbeta_score(predictions_test, y_test, beta = .5)\n",
    "       \n",
    "    # Success\n",
    "    print(\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # Return the results\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"init_eval\"></a>\n",
    "### Initial Model Evaluation\n",
    "In the next code cell, we implement the following:\n",
    "- Import the three supervised learning models we've discussed in the previous section.\n",
    "- Initialize the three models and store them in `'clf_A'`, `'clf_B'`, and `'clf_C'`.\n",
    "  - Use a `'random_state'` for each model if provided.\n",
    "- Calculate the number of records equal to 1%, 10%, and 100% of the training data.\n",
    "  - Store those values in `'samples_1'`, `'samples_10'`, and `'samples_100'` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the three supervised learning models from sklearn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = GaussianNB()\n",
    "clf_B = SVC(random_state = 42)\n",
    "clf_C = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# Calculate the number of samples for 1%, 10%, and 100% of the training data\n",
    "\n",
    "samples_100 = len(y_train)\n",
    "samples_10 = int(.1 * samples_100)\n",
    "samples_1 = int(.01 * samples_100)\n",
    "\n",
    "# Collect results on the learners\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, s in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = \\\n",
    "        train_predict(clf, s, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Run metrics visualization for the three supervised learning models chosen\n",
    "vs.evaluate(results, accuracy, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllDataResults = []\n",
    "for m in results.keys():\n",
    "    AllDataResults.append([results[m][2].get(key) for key in ['acc_test','f_test','pred_time']])\n",
    "\n",
    "AllDataResults = pd.DataFrame(AllDataResults, index = results.keys(), columns = ['Accuracy test', 'F test', 'Prediction time (s)'])\n",
    "AllDataResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"Improve\"></a>\n",
    "----\n",
    "## Improving Results\n",
    "Based on the initial results above, we choose the *best* model to use on the given data. Then, we perform a grid search optimization for this model over the entire training set (`X_train` and `y_train`) to improve upon the untuned model's F-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Best'></a>\n",
    "### Choosing the Best Model\n",
    "\n",
    "Based on the initial model evaluation results on the full data ,the `RandomForestClassifier` successfully prove its capability to produce more percise prediction in a little prediction time for large number of samples. Furthermore, the data has a lot of outliers which can be easily handled by tree-based models. Besides, the data has a lot of categorical features which can be another good reason to select tree models.  _For these reasons_, we select it as our best model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Describe'></a>\n",
    "### Describing the Model in Layman's Terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RandomForestClassifier` exploits the power of aggregating multiple weak classifiers in such a way to construct a strong classfier. By assigning a random small set of features to each learner, it can be able to learn the patterns hidden in those columns. Taking the teamwork of many trees thus improving the performance of a single random tree <sup>[Random forest](https://en.wikipedia.org/wiki/Random_forest).</sup> Basically, the weak learner is a decision tree which group similar data together based on a spicific condition. For example, the tree might want to divide the data points based on the education level feature and build further decisions upon completing this grouping. As a concrete example, if the data point has a bachelor degree then this point has more potential to have income greater than required threshold.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Tuning'></a>\n",
    "### Model Tuning\n",
    "Finally, we perform a grid search on the selected model to fine tune its parameters to improve the results as much as possible. \n",
    "\n",
    "**Note:** The following implementation may take several minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 'GridSearchCV', 'make_scorer', and any other necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "# Initialize the classifier\n",
    "clf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "# TODO: Create the parameters list you wish to tune, using a dictionary if needed.\n",
    "# HINT: parameters = {'parameter_1': [value1, value2], 'parameter_2': [value1, value2]}\n",
    "parameters = {'n_estimators' : [100, 200, 500] , 'max_depth' : [2,4,10], 'min_samples_split': [8,20], \n",
    "             'min_samples_leaf': [5,15]}\n",
    "\n",
    "# TODO: Make an fbeta_score scoring object using make_scorer()\n",
    "scorer = make_scorer(fbeta_score, beta=.5)\n",
    "\n",
    "# TODO: Perform grid search on the classifier using 'scorer' as the scoring method using GridSearchCV()\n",
    "grid_obj = GridSearchCV(estimator = clf, scoring = scorer, param_grid = parameters,verbose = 1,n_jobs = 2)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters using fit()\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "\n",
    "# Make predictions using the unoptimized and model\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_test)\n",
    "best_predictions = best_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the before-and-afterscores\n",
    "print(\"Unoptimized model\\n------\")\n",
    "print(\"Accuracy score on testing data: {:.4f}\".format(accuracy_score(y_test, predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, predictions, beta = 0.5)))\n",
    "print(\"\\nOptimized Model\\n------\")\n",
    "print(\"Final accuracy score on the testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(\"Final F-score on the testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'Final_Eval'></a>\n",
    "### Final Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a long time of searching, we managed to increase the overall performance of our model a little bit. The optimized model over perform the unoptimized model in both accuracy and F-score. Furthermore, the fine-tuned model super outperforms the naive predictor used as our baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FinalResults = pd.DataFrame(data = [[accuracy *100, accuracy_score(y_test, predictions)*100, accuracy_score(y_test, best_predictions) * 100],\n",
    "                               [fscore, fbeta_score(y_test, predictions, beta = 1), fbeta_score(y_test, best_predictions, beta = 0.5)]],\n",
    "                      index = ['Accuracy Score (%)', 'F-score [0 - 1]'], columns = ['Baseline Model', 'Unoptimized Model', 'Optimized Model'])\n",
    "FinalResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters found are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= \"Feature\"></a>\n",
    "----\n",
    "## Feature Importance\n",
    "\n",
    "An important task when performing supervised learning on a dataset like the census data we study here is determining which features provide the most predictive power. By focusing on the relationship between only a few crucial features and the target label we simplify our understanding of the phenomenon, which is most always a useful thing to do. In the case of this project, that means we wish to identify a small number of features that most strongly predict whether an individual makes at most or more than `$50,000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'f_obs'></a>\n",
    "### Feature Relevance Observation\n",
    "When **Exploring the Data**, it was shown there are thirteen available features for each individual on record in the census data. But of these thirteen records, which five features can we expect to be most important for prediction, and in what order can we rank them and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_To answer this question_, we refer back to assoication matrix calculated in <a href=\"#featureset\">Featureset Exploration </a>. Thus, we can expect the top 5 relevant features to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = associations(data,figsize = (20,15))\n",
    "default_corr = M['corr'].loc['Y']\n",
    "values = default_corr[np.argsort(default_corr)[::-1]]\n",
    "values[1:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, the `education level` as well as `education number` increase the person's cabability to land respectful jobs so it is logical that they are related to the target. In addition, the `occupation` provides helpful information about the current job which can be a good indicator too. Finally, the `marital status` and `relationship` can be a good indicator of the person maturity as well as current responsbilities which can provide helpful insights too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'f_ext'></a>\n",
    "### Extracting Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the feature importances using .feature_importances_ \n",
    "importances = best_clf.feature_importances_\n",
    "\n",
    "# Plot\n",
    "vs.feature_plot(importances, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "columns = X_train.columns.values[indices[:5]]\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracted Features Vs observed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RandomForestModel` agreed with our observation about importance of `Marital Status` as well as `education number` and `relationship` features. Furthermore, the model estimated which specific category of each of those features is the most important. For example, model assessed that Husband category can influence the prediction more than any other categories in relationships. However, the model differ with our observation about `occupation` and `education level`. Instead, it priortize the `capital gain` as well as `age` columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'f_sel'></a>\n",
    "### Feature Selection\n",
    "How does a model perform if we only use a subset of all the available features in the data? With less features required to train, the expectation is that training and prediction time is much lower — at the cost of performance metrics. From the visualization above, we see that the top five most important features contribute more than half of the importance of **all** features present in the data. This hints that we can attempt to *reduce the feature space* and simplify the information required for the model to learn. The code cell below use the same optimized model we found earlier, and train it on the same training set *with only the top five important features*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import functionality for cloning a model\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Reduce the feature space\n",
    "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "X_test_reduced = X_test[X_test.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "\n",
    "# Train on the \"best\" model found from grid search earlier\n",
    "clf = (clone(best_clf)).fit(X_train_reduced, y_train)\n",
    "\n",
    "# Make new predictions\n",
    "reduced_predictions = clf.predict(X_test_reduced)\n",
    "\n",
    "# Report scores from the final model using both versions of data\n",
    "print(\"Final Model trained on full data\\n------\")\n",
    "print(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, best_predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, best_predictions, beta = .5)))\n",
    "print(\"\\nFinal Model trained on reduced data\\n------\")\n",
    "print(\"Accuracy on testing data: {:.4f}\".format(accuracy_score(y_test, reduced_predictions)))\n",
    "print(\"F-score on testing data: {:.4f}\".format(fbeta_score(y_test, reduced_predictions, beta = .5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = 'f_eff'></a>\n",
    "### Effects of Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the feature selection worsen the performance a little bit but since we used more than half of the available information, we can see that final model do pretty well job on the reduced data. However, considering the tiny amount of time needed to train `RandomForestClassifiers`, we actually don't need this compromise. It can be useful if we selected `Support Vector Classifiers` because of their heavy training time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id =\"Conc\"></a>\n",
    "## Conclusion and future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A strong way to enhance your data science as well as machine learning skills is to participate in Kaggle competitions where you can practice the end-to-end process and see where you are stand in the leaderboard. This can be a strong motivation to keep improving your skills in order to compete effectively. In this notebook, we experienced the complexity of machine learning pipeline by applying several techniques on a real-world dataset. At the end of the day, we achieved a score of `76%` on the test data associated with this competition . A lot of work still needed on the pre-processing stage in order to enhance our score. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
